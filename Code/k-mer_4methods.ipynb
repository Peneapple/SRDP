{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9d57a0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('cds_structure_features_dic_1800.pkl', 'rb') as f:    \n",
    "    cds_3d_features_dic = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "27adb375",
   "metadata": {},
   "outputs": [],
   "source": [
    "intersection = set(feature_dic.keys()).intersection(reduced_feature_dic.keys(), cds_3d_features_dic.keys())\n",
    "\n",
    "common_list = list(intersection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5286fce2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17262"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(common_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149172de",
   "metadata": {},
   "source": [
    "Noticed that this common list is not common list in SRDP. Due to many sequence cannot be processed by 5-mer and 3-mer.\\\n",
    "This method loss more sample than SRDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c258a1de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data1/xpgeng/.conda/envs/pytorch/lib/python3.8/site-packages/openpyxl/worksheet/_reader.py:329: UserWarning: Unknown extension is not supported and will be removed\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data1/xpgeng/.conda/envs/pytorch/lib/python3.8/site-packages/openpyxl/worksheet/_reader.py:329: UserWarning: Unknown extension is not supported and will be removed\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# SR as '1' sample\n",
    "import pandas as pd\n",
    "SR = {}\n",
    "# Load spreadsheet\n",
    "xl = pd.ExcelFile('all_cancer_pair.xlsx')\n",
    "\n",
    "df = xl.parse(xl.sheet_names[0], skiprows=3)\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "# Use the first and second column values as a key, and set the value as 1\n",
    "    SR[row[0], row[1]] = 1\n",
    "\n",
    "    \n",
    "df = xl.parse(xl.sheet_names[2], skiprows=2)\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "# Use the first and second column values as a key, and set the value as 1\n",
    "    SR[row[0], row[1]] = 1\n",
    "\n",
    "print(len(SR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "43915389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018\n"
     ]
    }
   ],
   "source": [
    "nocds_SR = []\n",
    "for k,v in SR.items():\n",
    "    if k[0] not in common_list or k[1] not in common_list:\n",
    "        nocds_SR.append(k)\n",
    "for i in nocds_SR:\n",
    "    del SR[i]\n",
    "print(len(SR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6ffb5f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19853\n",
      "18161\n"
     ]
    }
   ],
   "source": [
    "# Non-SR as '0' sample\n",
    "from Bio import SeqIO\n",
    "\n",
    "def extract_gene_symbol(description):\n",
    "    parts = description.split(' ')\n",
    "    for part in parts:\n",
    "        if part.startswith('gene_symbol:'):\n",
    "            return part.split(':')[1]\n",
    "    return None\n",
    "\n",
    "fasta_file = \"Homocdsall.fasta\"  \n",
    "\n",
    "cds_dic = {}\n",
    "for record in SeqIO.parse(fasta_file, \"fasta\"):\n",
    "    gene_symbol = extract_gene_symbol(record.description)\n",
    "    cds_dic[gene_symbol] = str(record.seq)\n",
    "    \n",
    "cds_gene_list = []\n",
    "for k,v in cds_dic.items():       \n",
    "        cds_gene_list.append(k)\n",
    "cds_gene_list = list(set(cds_gene_list))\n",
    "print(len(cds_gene_list))\n",
    "\n",
    "import random\n",
    "\n",
    "Non_SR = {}\n",
    "random.seed(127)\n",
    "for i in range(20180-2018):\n",
    "    a = random.randint(0, 17261)\n",
    "    b = random.randint(0, 17261)\n",
    "    while a == b:\n",
    "        b = random.randint(0, 17261)\n",
    "    if (common_list[a], common_list[b]) not in SR:\n",
    "        \n",
    "        Non_SR[common_list[a], common_list[b]] = 0\n",
    "    \n",
    "print(len(Non_SR))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b90175",
   "metadata": {},
   "source": [
    "#### DNA 5-mer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52d061f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "\n",
    "def extract_gene_symbol(description):\n",
    "    parts = description.split(' ')\n",
    "    for part in parts:\n",
    "        if part.startswith('gene_symbol:'):\n",
    "            return part.split(':')[1]\n",
    "    return None  \n",
    "\n",
    "fasta_file = \"Homocdsall.fasta\"  \n",
    "\n",
    "cds_dic = {}\n",
    "for record in SeqIO.parse(fasta_file, \"fasta\"):\n",
    "    gene_symbol = extract_gene_symbol(record.description)\n",
    "    cds_dic[gene_symbol] = str(record.seq)\n",
    "    \n",
    "# Copy the dictionary to avoid issues with changing the dict size during iteration\n",
    "cds_dic_copy = cds_dic.copy()\n",
    "\n",
    "for gene_name, sequence in cds_dic_copy.items():\n",
    "    for char in sequence:\n",
    "        if char not in 'ATGC':\n",
    "            del cds_dic[gene_name]  # Remove this gene from the original dict\n",
    "            break  # No need to check the rest of the sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cc77ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Function to generate k-mers\n",
    "def generate_kmers(sequence, k):\n",
    "    return [sequence[i:i+k] for i in range(len(sequence) - k + 1)]\n",
    "\n",
    "# Generate 5-mers for each sequence in the dictionary\n",
    "k = 5\n",
    "kmers_dic = {gene_name: ' '.join(generate_kmers(seq, k)) for gene_name,\n",
    "             seq in cds_dic.items()}\n",
    "\n",
    "# Convert k-mers to feature vectors\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(kmers_dic.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eaea4d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Function to generate k-mers\n",
    "def generate_kmers(sequence, k):\n",
    "    return [sequence[i:i+k] for i in range(len(sequence) - k + 1)]\n",
    "\n",
    "\n",
    "# Generate 5-mers for each sequence in the dictionary\n",
    "k = 5\n",
    "kmers_dic = {gene_name: ' '.join(generate_kmers(seq, k)) for gene_name, seq in cds_dic.items()}\n",
    "\n",
    "# Convert k-mers to feature vectors\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(kmers_dic.values())\n",
    "\n",
    "# Create a dictionary with gene names as keys and feature vectors as values\n",
    "feature_dic = {gene_name: X[i].toarray()[0] for i, gene_name in enumerate(kmers_dic.keys())}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e4ce4f",
   "metadata": {},
   "source": [
    "#### Protein 3-mer PCA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3fe65c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "\n",
    "def extract_gene_symbol(description):\n",
    "    parts = description.split(' ')\n",
    "    for part in parts:\n",
    "        if part.startswith('gene_symbol:'):\n",
    "            return part.split(':')[1]\n",
    "    return None  \n",
    "\n",
    "fasta_file = \"Homopepall.fasta\" \n",
    "\n",
    "pep_dic = {}\n",
    "for record in SeqIO.parse(fasta_file, \"fasta\"):\n",
    "    gene_symbol = extract_gene_symbol(record.description)\n",
    "    pep_dic[gene_symbol] = str(record.seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e53b432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19853\n",
      "17511\n"
     ]
    }
   ],
   "source": [
    "# Copy the dictionary to avoid issues with changing the dict size during iteration\n",
    "pep_dic_copy = pep_dic.copy()\n",
    "print(len(pep_dic))\n",
    "for gene_name, sequence in pep_dic_copy.items():\n",
    "    for char in sequence:\n",
    "        if char not in 'ACDEFGHIKLMNPQRSTVWY':\n",
    "            del pep_dic[gene_name]  # Remove this gene from the original dict\n",
    "            break  # No need to check the rest of the sequence\n",
    "            \n",
    "print(len(pep_dic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5db9ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from itertools import product\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "# Generate all possible 3-mers\n",
    "amino_acids = 'ACDEFGHIKLMNPQRSTVWY'\n",
    "all_3mers = [''.join(p) for p in product(amino_acids, repeat=3)]\n",
    "\n",
    "def get_3mer_composition(protein_sequence):\n",
    "    # Generate 3-mers from the protein sequence\n",
    "    sequence_3mers = [protein_sequence[i:i+3] for i in range(len(protein_sequence) - 2)]\n",
    "    \n",
    "    # Count the frequency of each 3-mer\n",
    "    counter = Counter(sequence_3mers)\n",
    "    \n",
    "    # Create a dictionary with all 3-mers and their frequencies\n",
    "    composition = {k: counter.get(k, 0) for k in all_3mers}\n",
    "    \n",
    "    return composition\n",
    "\n",
    "# Compute the 3-mer composition for each protein\n",
    "tri_peptide_composition_dic = {protein_name: get_3mer_composition(seq) for protein_name, seq in pep_dic.items()}\n",
    "\n",
    "# Create a numpy array of the feature vectors\n",
    "feature_matrix = np.array([list(dic.values()) for dic in tri_peptide_composition_dic.values()])\n",
    "\n",
    "# Apply PCA to reduce dimensionality to 1000\n",
    "pca = PCA(n_components=1000)\n",
    "reduced_feature_matrix = pca.fit_transform(feature_matrix)\n",
    "\n",
    "# Map the reduced feature vectors back to the protein names\n",
    "reduced_feature_dic = {protein_name: reduced_feature_matrix[i] for i, protein_name in enumerate(tri_peptide_composition_dic.keys())}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c6adba",
   "metadata": {},
   "source": [
    "#### test performance DNA + Protein Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "96bcce38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████| 2018/2018 [00:00<00:00, 2145.36it/s]\n",
      "100%|█████████████████████████████████████████████████████| 18161/18161 [00:08<00:00, 2147.93it/s]\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "y = []\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "for gene_pair, label in tqdm(SR.items()):\n",
    "    try:       \n",
    "        a = feature_dic[gene_pair[0]]\n",
    "        b = feature_dic[gene_pair[1]]\n",
    "        c = reduced_feature_dic[gene_pair[0]]\n",
    "        d = reduced_feature_dic[gene_pair[1]]\n",
    "        e = cds_3d_features_dic[gene_pair[0]]\n",
    "        f = cds_3d_features_dic[gene_pair[1]]\n",
    "                \n",
    "        X.append(np.concatenate([a,b,c,d,e,f]))\n",
    "        y.append(label)        \n",
    "    except:\n",
    "        print(gene_pair)\n",
    "        \n",
    "for gene_pair, label in tqdm(Non_SR.items()):\n",
    "    try:       \n",
    "        a = feature_dic[gene_pair[0]]\n",
    "        b = feature_dic[gene_pair[1]]\n",
    "        c = reduced_feature_dic[gene_pair[0]]\n",
    "        d = reduced_feature_dic[gene_pair[1]]\n",
    "        e = cds_3d_features_dic[gene_pair[0]]\n",
    "        f = cds_3d_features_dic[gene_pair[1]]\n",
    "                \n",
    "        X.append(np.concatenate([a,b,c,d,e,f]))\n",
    "        y.append(label)        \n",
    "    except:\n",
    "        print(gene_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6ea85637",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93c75fa",
   "metadata": {},
   "source": [
    "Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4111afc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[[2530 1114]\n",
      " [ 175  216]]\n",
      "1\n",
      "[[2469 1165]\n",
      " [ 200  201]]\n",
      "2\n",
      "[[2509 1157]\n",
      " [ 168  201]]\n",
      "3\n",
      "[[2480 1113]\n",
      " [ 187  255]]\n",
      "4\n",
      "[[2381 1239]\n",
      " [ 204  211]]\n",
      "Average AUC: 0.6089820391442514\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# 合并X和y\n",
    "data = list(zip(X, y))\n",
    "\n",
    "# 打乱数据\n",
    "random.shuffle(data)\n",
    "\n",
    "# 将数据分成五部分\n",
    "n = len(data) // 5\n",
    "data_splits = [data[i * n:(i + 1) * n] for i in range(5)]\n",
    "\n",
    "# 保存所有的AUC\n",
    "auc_scores = []\n",
    "\n",
    "# 初始化高斯朴素贝叶斯模型\n",
    "gnb = GaussianNB()\n",
    "\n",
    "# 对每个分割，使用它作为测试集，其余的作为训练集\n",
    "for i in range(5):\n",
    "    print(i)\n",
    "    test_data = data_splits[i]\n",
    "    train_data = [item for sublist in data_splits[:i] + data_splits[i + 1:] for item in sublist]\n",
    "\n",
    "    # 分离X和y\n",
    "    X_test, y_test = zip(*test_data)\n",
    "    X_train, y_train = zip(*train_data)\n",
    "\n",
    "    # 训练模型\n",
    "    gnb.fit(X_train, y_train)\n",
    "\n",
    "    # 在测试集上预测\n",
    "    y_pred = gnb.predict(X_test)\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(cm)\n",
    "    \n",
    "    \n",
    "    # 计算并保存AUC\n",
    "    auc = roc_auc_score(y_test, y_pred)\n",
    "    auc_scores.append(auc)\n",
    "\n",
    "# 计算平均AUC\n",
    "average_auc = sum(auc_scores) / len(auc_scores)\n",
    "print(\"Average AUC:\", average_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884f139c",
   "metadata": {},
   "source": [
    "Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5fe3d8c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[[3623    6]\n",
      " [ 285  121]]\n",
      "1\n",
      "[[3650    5]\n",
      " [ 259  121]]\n",
      "2\n",
      "[[3629    8]\n",
      " [ 295  103]]\n",
      "3\n",
      "[[3636    3]\n",
      " [ 279  117]]\n",
      "4\n",
      "[[3591    8]\n",
      " [ 331  105]]\n",
      "Average AUC: 0.6403256618519786\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# 合并X和y\n",
    "data = list(zip(X, y))\n",
    "\n",
    "# 打乱数据\n",
    "random.shuffle(data)\n",
    "\n",
    "# 将数据分成五部分\n",
    "n = len(data) // 5\n",
    "data_splits = [data[i * n:(i + 1) * n] for i in range(5)]\n",
    "\n",
    "# 保存所有的AUC\n",
    "auc_scores = []\n",
    "\n",
    "# 初始化梯度提升模型\n",
    "gb = GradientBoostingClassifier()\n",
    "\n",
    "# 对每个分割，使用它作为测试集，其余的作为训练集\n",
    "for i in range(5):\n",
    "    print(i)\n",
    "    test_data = data_splits[i]\n",
    "    train_data = [item for sublist in data_splits[:i] + data_splits[i + 1:] for item in sublist]\n",
    "\n",
    "    # 分离X和y\n",
    "    X_test, y_test = zip(*test_data)\n",
    "    X_train, y_train = zip(*train_data)\n",
    "\n",
    "    # 训练模型\n",
    "    gb.fit(X_train, y_train)\n",
    "\n",
    "    # 在测试集上预测\n",
    "    y_pred = gb.predict(X_test)\n",
    "    \n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(cm)\n",
    "\n",
    "    # 计算并保存AUC\n",
    "    auc = roc_auc_score(y_test, y_pred)\n",
    "    auc_scores.append(auc)\n",
    "\n",
    "# 计算平均AUC\n",
    "average_auc = sum(auc_scores) / len(auc_scores)\n",
    "print(\"Average AUC:\", average_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f1dcb8",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8a6c6f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[[3609   14]\n",
      " [ 212  200]]\n",
      "1\n",
      "[[3623   21]\n",
      " [ 200  191]]\n",
      "2\n",
      "[[3590   19]\n",
      " [ 217  209]]\n",
      "3\n",
      "[[3622   17]\n",
      " [ 203  193]]\n",
      "4\n",
      "[[3630   12]\n",
      " [ 192  201]]\n",
      "Average AUC: 0.7440504168402348\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# 合并X和y\n",
    "data = list(zip(X, y))\n",
    "\n",
    "# 打乱数据\n",
    "random.shuffle(data)\n",
    "\n",
    "# 将数据分成五部分\n",
    "n = len(data) // 5\n",
    "data_splits = [data[i * n:(i + 1) * n] for i in range(5)]\n",
    "\n",
    "# 保存所有的AUC\n",
    "auc_scores = []\n",
    "\n",
    "# 初始化随机森林模型\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# 对每个分割，使用它作为测试集，其余的作为训练集\n",
    "for i in range(5):\n",
    "    print(i)\n",
    "    test_data = data_splits[i]\n",
    "    train_data = [item for sublist in data_splits[:i] + data_splits[i + 1:] for item in sublist]\n",
    "\n",
    "    # 分离X和y\n",
    "    X_test, y_test = zip(*test_data)\n",
    "    X_train, y_train = zip(*train_data)\n",
    "\n",
    "    # 训练模型\n",
    "    rf.fit(X_train, y_train)\n",
    "\n",
    "    # 在测试集上预测\n",
    "    y_pred = rf.predict(X_test)\n",
    "    \n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(cm)\n",
    "\n",
    "    # 计算并保存AUC\n",
    "    auc = roc_auc_score(y_test, y_pred)\n",
    "    auc_scores.append(auc)\n",
    "\n",
    "# 计算平均AUC\n",
    "average_auc = sum(auc_scores) / len(auc_scores)\n",
    "print(\"Average AUC:\", average_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8824ac",
   "metadata": {},
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9d144f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing fold 1\n",
      "[[3594   19]\n",
      " [ 237  185]]\n",
      "Processing fold 2\n",
      "[[3598   21]\n",
      " [ 221  195]]\n",
      "Processing fold 3\n",
      "[[3621   29]\n",
      " [ 231  154]]\n",
      "Processing fold 4\n",
      "[[3632   25]\n",
      " [ 210  168]]\n",
      "Processing fold 5\n",
      "[[3594   24]\n",
      " [ 226  191]]\n",
      "Average AUC: 0.8672551815543038\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix\n",
    "\n",
    "# Merge X and y\n",
    "data = list(zip(X, y))\n",
    "\n",
    "# Shuffle data\n",
    "random.shuffle(data)\n",
    "\n",
    "# Split data into five parts\n",
    "n = len(data) // 5\n",
    "data_splits = [data[i * n:(i + 1) * n] for i in range(5)]\n",
    "\n",
    "# Store all AUC scores\n",
    "auc_scores = []\n",
    "\n",
    "# Initialize SVM model\n",
    "svc = svm.SVC(probability=True)\n",
    "\n",
    "# For each split, use it as a test set, the rest as a training set\n",
    "for i in range(5):\n",
    "    print(f\"Processing fold {i+1}\")\n",
    "    test_data = data_splits[i]\n",
    "    train_data = [item for sublist in data_splits[:i] + data_splits[i + 1:] for item in sublist]\n",
    "\n",
    "    # Separate X and y\n",
    "    X_test, y_test = zip(*test_data)\n",
    "    X_train, y_train = zip(*train_data)\n",
    "\n",
    "    # Train model\n",
    "    svc.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on test set\n",
    "    y_pred_prob = svc.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Convert probabilities to class labels\n",
    "    y_pred = [1 if prob > 0.5 else 0 for prob in y_pred_prob]\n",
    "\n",
    "    # Generate confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(cm)\n",
    "\n",
    "    # Calculate and save AUC\n",
    "    auc = roc_auc_score(y_test, y_pred_prob)\n",
    "    auc_scores.append(auc)\n",
    "\n",
    "# Calculate average AUC\n",
    "average_auc = sum(auc_scores) / len(auc_scores)\n",
    "print(\"Average AUC:\", average_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab0c3c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-pytorch] *",
   "language": "python",
   "name": "conda-env-.conda-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
